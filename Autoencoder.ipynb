{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjK3BaeS7ujs"
      },
      "source": [
        "**Question 3**\n",
        "\n",
        "Use MNISTdataset and follow below instructions to solve this question.\n",
        "• Create a AutoEncoder with following instructions:\n",
        "– Create feed forward Neural Network as follows:\n",
        "∗ Input layer : (input = 784, output = 512,activation = ReLU)\n",
        "∗ Hidden layer : (input = 512, output = 128, activation = ReLU)\n",
        "∗ Latent Space : (input = 128,output = 64, activation = ReLU)\n",
        "∗ Hidden layer : (input = 64, output = 128,activation = ReLU)\n",
        "∗ Hidden layer :(input = 128,output = 512,activation = ReLU)\n",
        "∗ Output layer : (input = 512, output = ?,activation = ReLU)\n",
        "\n",
        "– Use appropriate loss function\n",
        "– Use Adam optimizer to optimize the loss function with proper learn-\n",
        "ing rate.\n",
        "– Use training data to train the autoencoder\n",
        "– After Training autoEncoder remove the decoder from the autoEn-\n",
        "coder architecture.\n",
        "\n",
        "• Create Classifcation Model called MNIST Classifcation Model with\n",
        "following configuration.\n",
        "• Use the encoder and then argument it with the following.\n",
        "– Input Layer :(input = 64 , output = 32 , activation = ReLU)\n",
        "– output Layer : (input = 32 , output = ? , activation = Softmax)\n",
        "• Use MultiClass Cross Entropy loss function and Adam optimizer to\n",
        "optimize the loss function with appropriate learning rate.\n",
        "\n",
        "• Train MNIST Classifcation Model using training dataset.Plot epoch-\n",
        "wise training loss.\n",
        "• Test using testing dataset and report accuracy and classwise accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z9qC6nec7xZL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import sys\n",
        "import pickle as cPickle\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "from keras import initializers\n",
        "from keras import optimizers\n",
        "from matplotlib import pyplot\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from array import array\n",
        "\n",
        "#Q3\n",
        "#unpacking the images\n",
        "def images_file_read(file_name):\n",
        "    with gzip.open(file_name, 'r') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        image_count = int.from_bytes(f.read(4), 'big')\n",
        "        row_count = int.from_bytes(f.read(4), 'big')\n",
        "        column_count = int.from_bytes(f.read(4), 'big')\n",
        "\n",
        "        image_data = f.read()\n",
        "        images = np.frombuffer(image_data, dtype=np.uint8)\\\n",
        "            .reshape((image_count, row_count, column_count))\n",
        "        return images\n",
        "\n",
        "def labels_file_read(file_name):\n",
        "    with gzip.open(file_name, 'r') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        label_count = int.from_bytes(f.read(4), 'big')\n",
        "        label_data = f.read()\n",
        "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
        "        return labels\n",
        "\n",
        "train_images = images_file_read(\"/content/drive/MyDrive/train-images-idx3-ubyte.gz\")\n",
        "train_labels = labels_file_read(\"/content/drive/MyDrive/train-labels-idx1-ubyte.gz\")\n",
        "test_imagesall = images_file_read(\"/content/drive/MyDrive/t10k-images-idx3-ubyte.gz\")\n",
        "test_labelsall = labels_file_read(\"/content/drive/MyDrive/t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "# dividing into validation set and test set\n",
        "val_images = test_imagesall[0:500,:,:]\n",
        "val_labels = test_labelsall[0:500]\n",
        "test_images = test_imagesall[500:,:,:]\n",
        "test_labels = test_labelsall[500:]\n",
        "\n",
        "#normalizing images\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "val_images = (val_images / 255) - 0.5\n",
        "\n",
        "#flatening the images\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "val_images = val_images.reshape((-1, 784))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4BDFynOJJx_f"
      },
      "outputs": [],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "input = layers.Dense(512, input_dim=784, activation='relu', trainable=True)(input_img)\n",
        "encoded = layers.Dense(128, activation='relu',trainable=True)(input)\n",
        "latent = layers.Dense(64, activation='relu',trainable=True)(encoded)\n",
        "\n",
        "decoded1 = layers.Dense(128, activation='relu',trainable=True)(latent)\n",
        "decoded2 = layers.Dense(512, activation='relu',trainable=True)(decoded1)\n",
        "output = layers.Dense(784, activation='relu',trainable=True)(decoded2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I4_hxCpeKZj2"
      },
      "outputs": [],
      "source": [
        "autoencoder = keras.Model(input_img, output)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "autoencoder.compile(optimizer=opt, loss='binary_crossentropy',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV6h1daQOA8R"
      },
      "outputs": [],
      "source": [
        "history=autoencoder.fit(train_images, train_images,\n",
        "                epochs=10,\n",
        "                batch_size=512,\n",
        "                shuffle=True,\n",
        "                validation_data=(val_images,val_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYTXTln3Op7T"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT9RQAdoQXHF",
        "outputId": "d0f33cae-a503-43e1-9934-57db04d7f111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 478,250\n",
            "Trainable params: 2,410\n",
            "Non-trainable params: 475,840\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "input = layers.Dense(512, input_dim=784, activation='relu', trainable=False)(input_img)\n",
        "encoded = layers.Dense(128, activation='relu',trainable=False)(input)\n",
        "latent = layers.Dense(64, activation='relu',trainable=False)(encoded)\n",
        "\n",
        "inp= layers.Dense(32,input_dim=64,activation='relu',trainable=True)(latent)\n",
        "out= layers.Dense(10,input_dim=32,activation='softmax',trainable=True)(inp)\n",
        "\n",
        "MNIST_Classification_Model= keras.Model(input_img,out)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "MNIST_Classification_Model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "MNIST_Classification_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anX7GMkTbliZ",
        "outputId": "ff21ea50-7f99-43ac-e306-e9238f86517b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "114/118 [===========================>..] - ETA: 0s - loss: 1.4454 - accuracy: 0.5496\n",
            "Epoch 1: val_accuracy improved from -inf to 0.67400, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.4339 - accuracy: 0.5529 - val_loss: 1.0157 - val_accuracy: 0.6740\n",
            "Epoch 2/30\n",
            "114/118 [===========================>..] - ETA: 0s - loss: 0.9568 - accuracy: 0.6906\n",
            "Epoch 2: val_accuracy improved from 0.67400 to 0.70000, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 0.9562 - accuracy: 0.6907 - val_loss: 0.9169 - val_accuracy: 0.7000\n",
            "Epoch 3/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.8996 - accuracy: 0.7071\n",
            "Epoch 3: val_accuracy did not improve from 0.70000\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.8997 - accuracy: 0.7071 - val_loss: 0.8980 - val_accuracy: 0.6980\n",
            "Epoch 4/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.8738 - accuracy: 0.7156\n",
            "Epoch 4: val_accuracy did not improve from 0.70000\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.8736 - accuracy: 0.7158 - val_loss: 0.8913 - val_accuracy: 0.6880\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - ETA: 0s - loss: 0.8567 - accuracy: 0.7210\n",
            "Epoch 5: val_accuracy improved from 0.70000 to 0.70600, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.8567 - accuracy: 0.7210 - val_loss: 0.8739 - val_accuracy: 0.7060\n",
            "Epoch 6/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.8407 - accuracy: 0.7255\n",
            "Epoch 6: val_accuracy improved from 0.70600 to 0.71000, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.8406 - accuracy: 0.7256 - val_loss: 0.8711 - val_accuracy: 0.7100\n",
            "Epoch 7/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.8250 - accuracy: 0.7307\n",
            "Epoch 7: val_accuracy improved from 0.71000 to 0.71600, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.8244 - accuracy: 0.7306 - val_loss: 0.8556 - val_accuracy: 0.7160\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.7350\n",
            "Epoch 8: val_accuracy did not improve from 0.71600\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.8105 - accuracy: 0.7350 - val_loss: 0.8553 - val_accuracy: 0.7020\n",
            "Epoch 9/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.7987 - accuracy: 0.7389\n",
            "Epoch 9: val_accuracy improved from 0.71600 to 0.72200, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 0.7990 - accuracy: 0.7388 - val_loss: 0.8364 - val_accuracy: 0.7220\n",
            "Epoch 10/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.7850 - accuracy: 0.7445\n",
            "Epoch 10: val_accuracy did not improve from 0.72200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7851 - accuracy: 0.7444 - val_loss: 0.8251 - val_accuracy: 0.7140\n",
            "Epoch 11/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.7748 - accuracy: 0.7475\n",
            "Epoch 11: val_accuracy improved from 0.72200 to 0.73400, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7751 - accuracy: 0.7473 - val_loss: 0.8273 - val_accuracy: 0.7340\n",
            "Epoch 12/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.7651 - accuracy: 0.7525\n",
            "Epoch 12: val_accuracy improved from 0.73400 to 0.73800, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7650 - accuracy: 0.7525 - val_loss: 0.8032 - val_accuracy: 0.7380\n",
            "Epoch 13/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.7522 - accuracy: 0.7564\n",
            "Epoch 13: val_accuracy improved from 0.73800 to 0.74200, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 0.7522 - accuracy: 0.7565 - val_loss: 0.7973 - val_accuracy: 0.7420\n",
            "Epoch 14/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.7441 - accuracy: 0.7582\n",
            "Epoch 14: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7446 - accuracy: 0.7581 - val_loss: 0.7970 - val_accuracy: 0.7400\n",
            "Epoch 15/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.7352 - accuracy: 0.7600\n",
            "Epoch 15: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7355 - accuracy: 0.7600 - val_loss: 0.7921 - val_accuracy: 0.7320\n",
            "Epoch 16/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.7263 - accuracy: 0.7635\n",
            "Epoch 16: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7266 - accuracy: 0.7637 - val_loss: 0.7782 - val_accuracy: 0.7280\n",
            "Epoch 17/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.7159 - accuracy: 0.7674\n",
            "Epoch 17: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7157 - accuracy: 0.7675 - val_loss: 0.7648 - val_accuracy: 0.7380\n",
            "Epoch 18/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.7694\n",
            "Epoch 18: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7073 - accuracy: 0.7695 - val_loss: 0.7790 - val_accuracy: 0.7300\n",
            "Epoch 19/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.7030 - accuracy: 0.7722\n",
            "Epoch 19: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.7018 - accuracy: 0.7724 - val_loss: 0.7537 - val_accuracy: 0.7300\n",
            "Epoch 20/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.6965 - accuracy: 0.7731\n",
            "Epoch 20: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6964 - accuracy: 0.7732 - val_loss: 0.7575 - val_accuracy: 0.7360\n",
            "Epoch 21/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.6892 - accuracy: 0.7762\n",
            "Epoch 21: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6896 - accuracy: 0.7760 - val_loss: 0.7686 - val_accuracy: 0.7360\n",
            "Epoch 22/30\n",
            "114/118 [===========================>..] - ETA: 0s - loss: 0.6841 - accuracy: 0.7764\n",
            "Epoch 22: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6851 - accuracy: 0.7761 - val_loss: 0.7657 - val_accuracy: 0.7260\n",
            "Epoch 23/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.7785\n",
            "Epoch 23: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6789 - accuracy: 0.7785 - val_loss: 0.7595 - val_accuracy: 0.7380\n",
            "Epoch 24/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.6744 - accuracy: 0.7801\n",
            "Epoch 24: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6746 - accuracy: 0.7800 - val_loss: 0.7441 - val_accuracy: 0.7380\n",
            "Epoch 25/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.6699 - accuracy: 0.7823\n",
            "Epoch 25: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6698 - accuracy: 0.7823 - val_loss: 0.7521 - val_accuracy: 0.7420\n",
            "Epoch 26/30\n",
            "116/118 [============================>.] - ETA: 0s - loss: 0.6627 - accuracy: 0.7835\n",
            "Epoch 26: val_accuracy did not improve from 0.74200\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6638 - accuracy: 0.7832 - val_loss: 0.7589 - val_accuracy: 0.7360\n",
            "Epoch 27/30\n",
            "114/118 [===========================>..] - ETA: 0s - loss: 0.6629 - accuracy: 0.7836\n",
            "Epoch 27: val_accuracy improved from 0.74200 to 0.75000, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6625 - accuracy: 0.7837 - val_loss: 0.7378 - val_accuracy: 0.7500\n",
            "Epoch 28/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.6565 - accuracy: 0.7851\n",
            "Epoch 28: val_accuracy did not improve from 0.75000\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6568 - accuracy: 0.7851 - val_loss: 0.7355 - val_accuracy: 0.7380\n",
            "Epoch 29/30\n",
            "117/118 [============================>.] - ETA: 0s - loss: 0.6537 - accuracy: 0.7869\n",
            "Epoch 29: val_accuracy did not improve from 0.75000\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6538 - accuracy: 0.7869 - val_loss: 0.7482 - val_accuracy: 0.7440\n",
            "Epoch 30/30\n",
            "115/118 [============================>.] - ETA: 0s - loss: 0.6503 - accuracy: 0.7878\n",
            "Epoch 30: val_accuracy improved from 0.75000 to 0.76200, saving model to best_model.h5\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 0.6503 - accuracy: 0.7879 - val_loss: 0.7313 - val_accuracy: 0.7620\n"
          ]
        }
      ],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "# Training the model.\n",
        "history=MNIST_Classification_Model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  validation_data=(val_images, to_categorical(val_labels)),  \n",
        "  epochs=30,\n",
        "  batch_size=512,\n",
        "  shuffle = True,\n",
        "  callbacks=[es,mc]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy24sLVEctR4",
        "outputId": "e8f2a3f3-4e58-4480-d4f4-19a866cf6048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "297/297 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.8057\n",
            "Classwise Accuracy:\n",
            "Class  0  : 0.8987206823027718\n",
            "Class  1  : 0.952247191011236\n",
            "Class  2  : 0.7584442169907881\n",
            "Class  3  : 0.7585492227979275\n",
            "Class  4  : 0.8435814455231931\n",
            "Class  5  : 0.665083135391924\n",
            "Class  6  : 0.8655737704918033\n",
            "Class  7  : 0.8304392236976507\n",
            "Class  8  : 0.7419700214132763\n",
            "Class  9  : 0.7130890052356021\n",
            "Overall accuracy: 0.8056842088699341\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model.\n",
        "saved_model = load_model('best_model.h5')\n",
        "scores = saved_model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "pred=MNIST_Classification_Model.predict(test_images) \n",
        "y_pred=np.argmax(pred,axis=1)\n",
        "\n",
        "matrix = confusion_matrix(test_labels, y_pred)\n",
        "cwa=matrix.diagonal()/matrix.sum(axis=1)\n",
        "\n",
        "print(\"Classwise Accuracy:\")\n",
        "for i in range(0,10):\n",
        "  print(\"Class \",i,\" :\",cwa[i])\n",
        "\n",
        "print('Overall accuracy:', scores[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SML_Assignment4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
